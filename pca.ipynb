{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and Covariance Matrices\n",
    "\n",
    "The cell below plots a small two-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[-4.40394652,  5.7203612 ],\n",
    "       [-5.4420319 ,  1.93587109],\n",
    "       [-4.56570001,  4.2530108 ],\n",
    "       [-5.07827013,  1.51210658],\n",
    "       [-5.39695317,  1.25160109],\n",
    "       [-4.28688083,  4.07841531],\n",
    "       [-3.58299231,  6.30384398],\n",
    "       [-5.60152663,  2.24470856],\n",
    "       [-4.47414671,  3.54114625],\n",
    "       [-5.00867538,  3.37502121],\n",
    "       [-3.61698704,  6.57347194],\n",
    "       [-7.14400941, -1.24682603],\n",
    "       [-8.34978399, -3.85127918],\n",
    "       [-7.35710211, -1.33622093],\n",
    "       [-2.82445297,  7.90898657],\n",
    "       [-4.65304533,  3.10063785],\n",
    "       [-3.41097162,  4.96990604],\n",
    "       [-5.91298693,  1.85557319],\n",
    "       [-6.65790912, -0.91626863],\n",
    "       [-3.67245142,  5.35009044]])\n",
    "\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell above, then answer the following questions.\n",
    "\n",
    "### Questions:\n",
    "*   Approximately where is the mean of this data set?  In other words, what is $[\\overline{x}_0, \\overline{x}_1]^T$ (where the subscripts corrspond to features)?\n",
    "*   If $\\mathbf{S}$ is the sample covariance matrix of this data set, \n",
    "    $$\\mathbf{S} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$$\n",
    "    * Which entry in $\\mathbf{S}$ corresponds to the variance of feature 0? \n",
    "    * Which entry in $\\mathbf{S}$ corresponds to the variance of feature 1? \n",
    "    * What is the value of $a$ relative to $d$?  Larger?  Smaller? Equal?\n",
    "    * What is the sign of $b$?\n",
    "    * What is the value of $b$ relative to $c$?  Larger? Smaller? Equal?\n",
    "*   If you could only keep one of the two features in this data set, which one would you pick.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "*     \n",
    "*    \n",
    "    * \n",
    "    * \n",
    "    * \n",
    "    * \n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering the data\n",
    "\n",
    "The first step in the PCA algorithm is to center the data by subtracting out the mean value of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=0)\n",
    "print(\"The mean is: \", X_mean)\n",
    "\n",
    "X = X - X_mean # Center the data by subtracting out the mean.\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Sample Covariance Matrix\n",
    "\n",
    "The next step is to calculate the sample covariance matrix.  The sample covariance matrix can be expressed as: \n",
    "\n",
    "$$\\mathbf{S} = \\frac{1}{n-1} \\sum_1^n (X_i - \\overline{X})(X_i - \\overline{X})^T $$\n",
    "\n",
    "Where $X_i$ represents the $i$th row of $X$ and $\\overline{X}$ is the sample mean. \n",
    "\n",
    "Of course... In this case, we've already centered the data, so $\\overline{X}$ is $\\mathbf{0}$ and the formula reduces to:\n",
    "\n",
    "$$\\mathbf{S} = \\frac{1}{n-1} \\sum_1^n X_i X_i^T $$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\\mathbf{S} = \\frac{1}{n-1} X^T X $$\n",
    "\n",
    "Where $X$ is the full, centered, data matrix.  Lets see if it works!\n",
    "\n",
    "Once you've executed the cell below, go back and check the answers you entered above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] # grab the number of samples\n",
    "\n",
    "S = np.dot(X.T, X) / (n-1)\n",
    "\n",
    "print(\"X.T times X:\\n\", S)\n",
    "\n",
    "print(\"\\nNumpy's version:\\n\", np.cov(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors\n",
    "\n",
    "The *eigenvectors* of a matrix are special vectors that don't change direction when multiplied by that matrix. In other words, multiplying a matrix by one of its eigenvectors gives the same answer as multiplying by some scalar:\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "Here the scalar $\\lambda$ is the known as an *eigenvalue*.\n",
    "\n",
    "Consider the matrix $$\\mathbf{A} = \\begin{bmatrix} 1 & .5 \\\\ .5 & 1 \\end{bmatrix}$$\n",
    "\n",
    "Its eigenvectors are:\n",
    "\n",
    "$$\\mathbf{v}_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}, ~~\n",
    "\\mathbf{v}_2 = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "### Question\n",
    "\n",
    "* Calculate by hand $\\mathbf{A} \\mathbf{v}_1$ and $\\mathbf{A} \\mathbf{v}_1$.  What are $\\lambda_1$ and $\\lambda_2$?\n",
    "\n",
    "### Answer\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "(Note that these two eigenvectors aren't unique.  We can rescale them, and the results will still be eigenvectors.  It is common to use eigenvectors that are scaled to unit length.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components\n",
    "\n",
    "It turns out that we can find the directions of maximum variance in a dataset by finding the eigenvectors of the covariance matrix.  The eigenvector with the largest eigenvalue points in the direction of maximum variance. Before executing the next cell, take a look back at the dataset above to predict the direction of the first principal component. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numpy is able to calculate eigenvalues and eigenvectors.\n",
    "#   `lambdas` contains the eigenvalues (in sorted order)\n",
    "#    The columms of `W` contain the corresponding eigenvectors.\n",
    "lambdas, W = np.linalg.eigh(S)\n",
    "\n",
    "# Lets reverse the order to get the largest first...\n",
    "\n",
    "lambdas = lambdas[::-1]\n",
    "W = W[:, ::-1]\n",
    "\n",
    "print(\"Eigenvalues:\", lambdas)\n",
    "print(\"Eigenvectors:\\n\", W)\n",
    "\n",
    "# Replot the data points\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Show the first principal component.\n",
    "plt.arrow(0, 0, W[0,0] * 3, W[1,0] * 3, head_width=0.5, head_length=0.5)\n",
    "plt.title('First Principal Component')\n",
    "plt.show()\n",
    "\n",
    "# Show the second principal component.\n",
    "plt.figure()\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.arrow(0, 0, W[0,1] * 3, W[1,1] * 3, head_width=0.5, head_length=0.5)\n",
    "plt.title('Second Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage Variance\n",
    "\n",
    "The magnitude of the eigenvectors are exactly equal to the amount of variance along the corresponding eigenvector.  If we normalize the eigenvalues, they give us the fraction of the total variance explained by each principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambdas / np.sum(lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, around 99% of the variance is captured by the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Data Onto Principal Components\n",
    "\n",
    "Once we have our principal components, we would like to know were each data point \"lands\" on each component.  In other words, we would like to project our data onto the principal components:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/300px-Dot_Product.svg.png\">\n",
    "\n",
    "Here **A** corresponds to a data point and **B** corresponds to a principal component.  In the case where **B** is a unit vector, $|\\mathbf{A}| \\cos(\\theta) = \\mathbf{A} \\cdot \\mathbf{B}$.  In othewords, the projection of **A** onto **B** is just the dot product between the two.  Using this information, we can project any one of our data points onto our first principal component. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the original data and the PC\n",
    "plt.axis('equal')\n",
    "plt.plot(X[:, 0], X[:, 1], '.', alpha=.1)\n",
    "plt.arrow(0, 0, W[0,0]*5, W[1,0]*5, head_width=0.5, head_length=0.5)\n",
    "\n",
    "# Pull out a single poing and project it onto the first PC:\n",
    "x0 = X[0,:]\n",
    "x0_projected = np.dot(x0, W[:,0])\n",
    "print(\"Projection: \", x0_projected)\n",
    "\n",
    "# Plot the original point\n",
    "plt.plot(x0[0], x0[1], 'og')\n",
    "\n",
    "# Plot where the original point is projected onto the PC:\n",
    "plt.plot(W[0,0] * x0_projected, W[1,0] * x0_projected, 'or')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(x0_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting ALL the data\n",
    "\n",
    "We can get a rotated version of our original, centered, dataset by projecting every data point onto our two principal components.  This can be accomplished at one go by multiplying our data matrix by our matrix of principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.dot(X, W)\n",
    "# Replot the data points\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, the x-axis shows the projection of the data onto the first principal component, and the y-axis shows the projection of the data onto the second principal component.\n",
    "\n",
    "### Questions:\n",
    "You should be able to predict all of the values in the covariance matrix of this rotated version of the data without doing any additional calculations.  What are they?\n",
    "$$\\mathbf{S} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$$\n",
    " * $a$: \n",
    " * $b$: \n",
    " * $c$: \n",
    " * $d$: \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is All of This Useful?\n",
    "\n",
    "In the example above, 99% of the variance is explained by the first principal component.  Now that the data is aligned with the principal components, we can discard the second feature, and in a sense, we are only losing 1% of the information from our original dataset. Discarding dimensions in this way can be helpful for a couple of different reasons:\n",
    "\n",
    "### Visualization\n",
    "\n",
    "Data that has four or more dimensions is essentially impossible to visualize using traditional techiques.  Using PCA we can reduce our high-dimensional data down to two or three dimensions for the purposes of visualization.  Depending on how much variance is explained by those first few dimensions, we may lose very little meaningful structure when moving to the low-dimensional space. \n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Some machine learning algorithms will operate more efficiently, or give better results, when the dimensionality of the input data is low. PCA is a common pre-processing step for discarding \"useless\" dimensions before applying these algorithms.\n",
    "\n",
    "## A Note on Implementation\n",
    "\n",
    "The implementation outlined above breaks down for high-dimensional data.  This is a problem because we may sometimes want to perform PCA on *very* high-dimensional data like images.\n",
    "\n",
    "### Question\n",
    "* *Why* does the algorithm outlined above break down for high dimensional data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would *not* find the principal components by explicitly calculating the sample covariance matrix. Instead we would use *Singular Value Decomposition*.  Google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
