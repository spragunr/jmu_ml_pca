{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Scale PCA Example and the Limits of PCA\n",
    "\n",
    "The previous notebook discussed the mechanics of PCA by looking at a two-dimensional dataset.  PCA is generally more valuable when applied to high-dimensional data.  This notebook will illustrate the use of PCA to reduce the dimensionity of image data.\n",
    "\n",
    "\n",
    "## PCA on MNIST Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "\n",
    "\n",
    "def load_mnist(split='train'):\n",
    "   \n",
    "    dataset = tfds.load(name=\"mnist\", split=split)\n",
    "    ds_numpy = tfds.as_numpy(dataset)\n",
    "        \n",
    "    images = np.array([ex['image'] for ex in ds_numpy])\n",
    "    dataset = tfds.load(name=\"mnist\", split=split)\n",
    "    ds_numpy = tfds.as_numpy(dataset)\n",
    "    labels = np.array([ex['label'] for ex in ds_numpy])\n",
    "\n",
    "    # Rescale and convert to floats...\n",
    "    result_images = np.array(images / 255.0, dtype=np.float32)\n",
    "    result_labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "    return result_images, result_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = load_mnist()\n",
    "print(X.shape)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree Plots\n",
    "\n",
    "Execute the cell below to visualize the amount of variance captured by each of the principal components for the MNIST data set.  Around 90% of the variance is captured by the first 100 principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_, '.-')\n",
    "plt.title('PC Variance')\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), '.-')\n",
    "plt.title('Cumulative PC Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the First Two Principal Components\n",
    "\n",
    "The figure below shows all of the 0's, 1's and 2's in the data set projected onto the first two principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pca.transform(X)\n",
    "#plt.plot(X_low[:,0], X_low[:,1], '.')\n",
    "plt.plot(Z[labels==0, 0], Z[labels==0, 1],'.r', alpha=.1)\n",
    "plt.plot(Z[labels==1, 0], Z[labels==1, 1],'.g', alpha=.1)\n",
    "plt.plot(Z[labels==2, 0], Z[labels==2, 1],'.b', alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA For Image Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "pca.fit(X)\n",
    "Z50 = pca.transform(X)\n",
    "X_back = pca.inverse_transform(Z50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_back = X_back.reshape(-1, 28, 28)\n",
    "X_img = X.reshape(-1, 28, 28)\n",
    "plt.imshow(X_img[0,:,:],cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(X_back[0,:,:],cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Does PCA Fall Short?\n",
    "\n",
    "Consider the following dataset.  Take a second to predict what will happen when we use PCA to project this to one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.multivariate_normal([-5, 3], [[1.5, 3], [3, 6.5]], 200)\n",
    "X2 = np.random.multivariate_normal([-6.5, 5], [[1.5, 3], [3, 6.5]], 200)\n",
    "X = np.vstack((X1, X2))\n",
    "np.random.shuffle(X)\n",
    "plt.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "Z1 = pca.transform(X)\n",
    "X_back = pca.inverse_transform(Z1)\n",
    "np.random.shuffle(X)\n",
    "plt.plot(X_back[:, 0], X_back[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the story: The directions of maximum variance aren't always the most meaningful dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Structure\n",
    "\n",
    "Execute the cell below to visualize some additional data with structure that is not well captured by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = sklearn.datasets.make_moons(n_samples=1000, noise=.05)[0]\n",
    "circles = sklearn.datasets.make_circles(n_samples=1000, factor=.5,\n",
    "                                noise=.05)[0]\n",
    "\n",
    "plt.plot(moons[:, 0], moons[:, 1], '.')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(circles[:, 0], circles[:, 1], '.')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X, color = sklearn.datasets.make_swiss_roll(n_samples=1500)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA is a linear method\n",
    "\n",
    "Linear methods *can* rotate, shear, scale, project, but they *can't* unroll.\n",
    "\n",
    "For that we need non-linear dimensionality reduction techniques:\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
